{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "brt = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "message = \"hi, how are you?\\n\"\n",
    "\n",
    "# system_message = '''You are hired as a data scientist on a new VALORANT esports team and \n",
    "#     have been tasked by the team’s general manager to support the scouting and recruitment \n",
    "#     process, please answer the above question.\n",
    "#     '''\n",
    "# message += system_message\n",
    "\n",
    "body = json.dumps({\n",
    "    \"inputText\": message,\n",
    "    \"textGenerationConfig\": {\n",
    "        \"maxTokenCount\": 2048,\n",
    "        \"stopSequences\": [\"User:\"],\n",
    "        \"temperature\": 0,\n",
    "        \"topP\": 0.9\n",
    "    }\n",
    "})\n",
    "\n",
    "modelId = 'amazon.titan-text-express-v1'\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "\n",
    "response = brt.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "\n",
    "response_body = json.loads(response.get('body').read())\n",
    "\n",
    "output_text = response_body['results'][0]['outputText']\n",
    "\n",
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputTextTokenCount': 7,\n",
       " 'results': [{'tokenCount': 9,\n",
       "   'outputText': 'Hello! How can I help you?',\n",
       "   'completionReason': 'FINISH'}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_body = json.loads(response.get('body').read())\n",
    "response_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I help you?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从response_body中获取outputText\n",
    "output_text = response_body['results'][0]['outputText']\n",
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The VALORANT esports team is looking to hire a data scientist to support the scouting and recruitment process. The general manager has tasked the data scientist with providing insights and analysis to help the team identify and recruit talented players. The data scientist will work closely with the team’s coaches and management to gather and analyze player data, including game statistics, performance metrics, and video footage. They will use advanced analytics techniques to identify patterns and trends in player performance and develop predictive models to help the team make informed decisions about player recruitment. The data scientist will also be responsible for developing and maintaining a database of player information and statistics, which will be used by the team’s coaching staff and management to track player progress and make adjustments to the team’s strategy. In addition to their technical skills, the data scientist will need to have strong communication and interpersonal skills, as they will be working closely with the team’s coaches and management to understand their needs and provide insights that can help the team succeed. They will also need to be able to work under pressure and meet tight deadlines, as the scouting and recruitment process is critical to the team’s success. Overall, the data scientist will play a critical role in helping the VALORANT esports team identify and recruit talented players, and will be a key member of the team’s coaching and management staff."
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "brt = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "message = \"hi, how are you?\\n\"\n",
    "\n",
    "system_message = '''You are hired as a data scientist on a new VALORANT esports team and \n",
    "    have been tasked by the team’s general manager to support the scouting and recruitment \n",
    "    process, please answer the above question.\n",
    "    '''\n",
    "\n",
    "message += system_message\n",
    "\n",
    "body = json.dumps({\n",
    "    \"inputText\": message,\n",
    "    \"textGenerationConfig\": {\n",
    "        \"maxTokenCount\": 2048,\n",
    "        \"stopSequences\": [\"User:\"],\n",
    "        \"temperature\": 0,\n",
    "        \"topP\": 0.9\n",
    "    }\n",
    "})\n",
    "\n",
    "modelId = 'amazon.titan-text-express-v1'\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "\n",
    "response = brt.invoke_model_with_response_stream(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "\n",
    "stream = response.get('body')\n",
    "if stream:\n",
    "    for event in stream:\n",
    "        chunk = event.get('chunk')\n",
    "        print(chunk)\n",
    "        if chunk:\n",
    "            print(json.loads(chunk.get('bytes')).get('outputText'), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating message with model meta.llama3-70b-instruct-v1:0\n",
      "INFO:__main__:Input tokens: 49\n",
      "INFO:__main__:Output tokens: 49\n",
      "INFO:__main__:Total tokens: 98\n",
      "INFO:__main__:Stop reason: end_turn\n",
      "INFO:__main__:Generating message with model meta.llama3-70b-instruct-v1:0\n",
      "INFO:__main__:Input tokens: 118\n",
      "INFO:__main__:Output tokens: 51\n",
      "INFO:__main__:Total tokens: 169\n",
      "INFO:__main__:Stop reason: end_turn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role: user\n",
      "Text: Create a list of 3 pop songs.\n",
      "\n",
      "Role: assistant\n",
      "Text: \n",
      "\n",
      "Here are 3 pop songs:\n",
      "\n",
      "1. \"Happy\" by Pharrell Williams\n",
      "2. \"Uptown Funk\" by Mark Ronson ft. Bruno Mars\n",
      "3. \"Can't Stop the Feeling!\" by Justin Timberlake\n",
      "\n",
      "Role: user\n",
      "Text: Make sure the songs are by artists from the United Kingdom.\n",
      "\n",
      "Role: assistant\n",
      "Text: \n",
      "\n",
      "Here are 3 pop songs by artists from the United Kingdom:\n",
      "\n",
      "1. \"Wannabe\" by Spice Girls\n",
      "2. \"Back for Good\" by Take That\n",
      "3. \"Price Tag\" by Jessie J ft. B.o.B\n",
      "\n",
      "Finished generating text with model meta.llama3-70b-instruct-v1:0.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import boto3\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def generate_conversation(bedrock_client,\n",
    "                          model_id,\n",
    "                          system_prompts,\n",
    "                          messages):\n",
    "    \"\"\"\n",
    "    Sends messages to a model.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        system_prompts (JSON) : The system prompts for the model to use.\n",
    "        messages (JSON) : The messages to send to the model.\n",
    "\n",
    "    Returns:\n",
    "        response (JSON): The conversation that the model generated.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Generating message with model %s\", model_id)\n",
    "\n",
    "    # Inference parameters to use.\n",
    "    temperature = 0.5\n",
    "    top_k = 0.9\n",
    "\n",
    "    # Base inference parameters to use.\n",
    "    inference_config = {\"temperature\": temperature}\n",
    "    # Additional inference parameters to use.\n",
    "    additional_model_fields = {\"top_k\": top_k}\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config\n",
    "        # additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "\n",
    "    # Log token usage.\n",
    "    token_usage = response['usage']\n",
    "    logger.info(\"Input tokens: %s\", token_usage['inputTokens'])\n",
    "    logger.info(\"Output tokens: %s\", token_usage['outputTokens'])\n",
    "    logger.info(\"Total tokens: %s\", token_usage['totalTokens'])\n",
    "    logger.info(\"Stop reason: %s\", response['stopReason'])\n",
    "\n",
    "    return response\n",
    "\n",
    "def main():\n",
    "    model_id = 'meta.llama3-70b-instruct-v1:0'\n",
    "\n",
    "    # Setup the system prompts and messages to send to the model.\n",
    "    system_prompts = [{\"text\": \"You are an app that creates playlists for a radio station that plays rock and pop music.\"\n",
    "                       \"Only return song names and the artist.\"}]\n",
    "    message_1 = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": \"Create a list of 3 pop songs.\"}]\n",
    "    }\n",
    "    message_2 = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": \"Make sure the songs are by artists from the United Kingdom.\"}]\n",
    "    }\n",
    "    messages = []\n",
    "\n",
    "    try:\n",
    "\n",
    "        bedrock_client = boto3.client(service_name='bedrock-runtime', region_name=\"us-east-1\")\n",
    "\n",
    "        # Start the conversation with the 1st message.\n",
    "        messages.append(message_1)\n",
    "        response = generate_conversation(\n",
    "            bedrock_client, model_id, system_prompts, messages)\n",
    "\n",
    "        # Add the response message to the conversation.\n",
    "        output_message = response['output']['message']\n",
    "        messages.append(output_message)\n",
    "\n",
    "        # Continue the conversation with the 2nd message.\n",
    "        messages.append(message_2)\n",
    "        response = generate_conversation(\n",
    "            bedrock_client, model_id, system_prompts, messages)\n",
    "\n",
    "        output_message = response['output']['message']\n",
    "        messages.append(output_message)\n",
    "\n",
    "        # Show the complete conversation.\n",
    "        for message in messages:\n",
    "            print(f\"Role: {message['role']}\")\n",
    "            for content in message['content']:\n",
    "                print(f\"Text: {content['text']}\")\n",
    "            print()\n",
    "\n",
    "    except ClientError as err:\n",
    "        message = err.response['Error']['Message']\n",
    "        logger.error(\"A client error occurred: %s\", message)\n",
    "        print(f\"A client error occured: {message}\")\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            f\"Finished generating text with model {model_id}.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating message with model meta.llama3-70b-instruct-v1:0\n",
      "INFO:__main__:Input tokens: 49\n",
      "INFO:__main__:Output tokens: 53\n",
      "INFO:__main__:Total tokens: 102\n",
      "INFO:__main__:Stop reason: end_turn\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '5ccf93fc-814f-4d5c-97ce-5f0fe152adc2',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Tue, 22 Oct 2024 12:53:40 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '371',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '5ccf93fc-814f-4d5c-97ce-5f0fe152adc2'},\n",
       "  'RetryAttempts': 0},\n",
       " 'output': {'message': {'role': 'assistant',\n",
       "   'content': [{'text': '\\n\\nHere are 3 pop songs for your radio station:\\n\\n1. \"Happy\" by Pharrell Williams\\n2. \"Uptown Funk\" by Mark Ronson ft. Bruno Mars\\n3. \"Can\\'t Stop the Feeling!\" by Justin Timberlake'}]}},\n",
       " 'stopReason': 'end_turn',\n",
       " 'usage': {'inputTokens': 49, 'outputTokens': 53, 'totalTokens': 102},\n",
       " 'metrics': {'latencyMs': 1196}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_conversation(bedrock_client,\n",
    "                          model_id,\n",
    "                          system_prompts,\n",
    "                          messages):\n",
    "    \"\"\"\n",
    "    Sends messages to a model.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        system_prompts (JSON) : The system prompts for the model to use.\n",
    "        messages (JSON) : The messages to send to the model.\n",
    "\n",
    "    Returns:\n",
    "        response (JSON): The conversation that the model generated.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Generating message with model %s\", model_id)\n",
    "\n",
    "    # Inference parameters to use.\n",
    "    temperature = 0.5\n",
    "    top_k = 0.9\n",
    "\n",
    "    # Base inference parameters to use.\n",
    "    inference_config = {\"temperature\": temperature}\n",
    "    # Additional inference parameters to use.\n",
    "    additional_model_fields = {\"top_k\": top_k}\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config\n",
    "        # additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "\n",
    "    # Log token usage.\n",
    "    token_usage = response['usage']\n",
    "    logger.info(\"Input tokens: %s\", token_usage['inputTokens'])\n",
    "    logger.info(\"Output tokens: %s\", token_usage['outputTokens'])\n",
    "    logger.info(\"Total tokens: %s\", token_usage['totalTokens'])\n",
    "    logger.info(\"Stop reason: %s\", response['stopReason'])\n",
    "\n",
    "    return response\n",
    "\n",
    "model_id = \"meta.llama3-70b-instruct-v1:0\"\n",
    "bedrock_client = boto3.client(service_name='bedrock-runtime', region_name=\"us-east-1\")\n",
    "\n",
    "system_message = '''\n",
    "You are an app that creates playlists for a radio station that plays rock and pop music.\n",
    "Only return song names and the artist.\n",
    "'''\n",
    "system_message = [{\"text\": system_message}]\n",
    "\n",
    "message_1 = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": \"Create a list of 3 pop songs.\"}]\n",
    "}\n",
    "\n",
    "messages = []\n",
    "messages.append(message_1)\n",
    "\n",
    "response = generate_conversation(\n",
    "    bedrock_client, model_id, system_message, messages)\n",
    "\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nHere are 3 pop songs for your radio station:\\n\\n1. \"Happy\" by Pharrell Williams\\n2. \"Uptown Funk\" by Mark Ronson ft. Bruno Mars\\n3. \"Can\\'t Stop the Feeling!\" by Justin Timberlake'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['output']['message']['content'][0]['text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
